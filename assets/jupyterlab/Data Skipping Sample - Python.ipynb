{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Data Skipping Sample - Python"}, {"cell_type": "markdown", "metadata": {}, "source": "[Data skipping](#) can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on a summary metadata associated with each object.\n\nFor every column in the object, the summary metadata might include minimum and maximum values, a list or bloom filter of the appearing values, or other metadata which succinctly represents the data in that column. This metadata is used during query evaluation to skip over objects which have no relevant data.\n\nAll Spark native data formats are supported, including Parquet, ORC, CSV, JSON and Avro. Data skipping is a performance optimization feature which means that using data skipping does not affect the content of the query results.\n\nTo use this feature, you need to create indexes on one or more columns of the data set. After this is done, Spark SQL queries can benefit from data skipping. In general, you should index the columns which are queried most often in the WHERE clause."}, {"cell_type": "markdown", "metadata": {}, "source": "## Setup the environment"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from metaindex import MetaIndexManager"}, {"cell_type": "markdown", "metadata": {}, "source": "**(optional)** set log level to DEBUG for the metaindex package - to view the skipped objects "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "log4jLogger = spark.sparkContext._jvm.org.apache.log4j\nlog4jLogger.LogManager.getLogger('com.ibm.metaindex.search').setLevel(log4jLogger.Level.DEBUG)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Configure Stocator\nFor more info on how to config credentials see [here](https://github.com/CODAIT/stocator)\n\nSee [here](https://cloud.ibm.com/docs/services/cloud-object-storage?topic=cloud-object-storage-endpoints) for the list of endpoints.\nMake sure you choose the private endpoint of your bucket"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "hconf = spark.sparkContext._jsc.hadoopConfiguration()\nhconf.set(\"fs.cos.service.endpoint\" ,\"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\")\nhconf.set(\"fs.cos.service.access.key\", \"<accessKey>\")\nhconf.set(\"fs.cos.service.secret.key\",\"<secretKey>\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Setup the DataSkipping library\nIn this example, we will set the JVM wide parameter to a base path to store all of the indexes. \n\nMetadata can be stored on the same storage system as the data however, not under the same path. For more configuration options, see [Data skipping configuration options](#)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "md_base_location = \"cos://mybucket.service/location/to/my/base/metdata\"\nMetaIndexManager.setDefaultMetaDataStore(spark, 'com.ibm.metaindex.metadata.metadatastore.parquet.Parquet')\nmd_backend_config = dict([('spark.ibm.metaindex.parquet.mdlocation', md_base_location),\n(\"spark.ibm.metaindex.parquet.mdlocation.type\", \"EXPLICIT_BASE_PATH_LOCATION\")])\nMetaIndexManager.setConf(spark, md_backend_config)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Indexing a dataset\n\nSkip this step if the data set is already indexed.\n\nNote that each of the index types has a corresponding method in the indexBuilder class of the form:\n\n`add[IndexType]Index(<index_params>)`\n\nFor example:\n\n`addMinMaxIndex(col: String)`\n\n`addValueListIndex(col: String)`\n\n`addBloomFilterIndex(col: String)`"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "dataset_location = \"cos://mybucket.service/location/to/my/data\"\nmd_backend = 'com.ibm.metaindex.metadata.metadatastore.parquet.ParquetMetadataBackend'\nreader = spark.read.format(\"parquet\")\nim = MetaIndexManager(spark, dataset_location, md_backend)\n\n# remove existing index first\nif im.isIndexed():\n\tim.removeIndex()\n    \n# indexing\nprint(\"Building the index:\")\nim.indexBuilder()\\\n  .addMinMaxIndex(\"temp\")\\\n  .addValueListIndex(\"city\")\\\n  .addBloomFilterIndex(\"vid\")\\\n  .build(reader)\\\n  .show(10, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "**(optional)** to refresh an indexed dataset use"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "im.refreshIndex(reader).show(10, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "view index status"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "im.indexStats().show(10, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Using the data skipping indexes "}, {"cell_type": "markdown", "metadata": {}, "source": "### Injecting the data skipping rule and enabling data skipping\nthe rule injection should be done only once per Spark session."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# inject the data skipping rule\nMetaIndexManager.injectDataSkippingRule(spark)\n\n# enable data skipping\nMetaIndexManager.enableFiltering(spark)\n\n# you can disable the data skipping any time by running: \n# MetaIndexManager.disableFiltering(spark)`"}, {"cell_type": "markdown", "metadata": {}, "source": "## Run a query"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df = reader.load(dataset_location)\ndf.createOrReplaceTempView(\"metergen\")\nspark.sql(\"select count(*) from metergen where temp > 30\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## View the data skipping statistics"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "MetaIndexManager.getLatestQueryAggregatedStats(spark).show(10, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "**(optional)** clear the stats for the next query (otherwise, stats will acummulate)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "MetaIndexManager.clearStats(spark)"}], "metadata": {"kernelspec": {"display_name": "Python 3.6 with Spark", "language": "python3", "name": "python36"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 1}